\section{Results}
\label{sec:results}

\subsection{Training Loss Follows Expected Scaling}
\label{sec:loss_scaling}

As a sanity check, we verify that our model family exhibits the expected power-law scaling of loss with model size.
Final validation loss scales as $L(N) \propto N^{-0.144}$ with $R^2 = 0.997$ and Spearman $\rho = -1.00$ ($p < 10^{-24}$).
All five models achieve monotonically decreasing loss curves, with larger models reaching lower final loss (\figref{fig:summary}, \figtop \figleft).

\subsection{$\Hzero$ Total Persistence Strongly Tracks Loss}
\label{sec:h0_results}

\begin{table}[t]
    \centering
    \caption{Spearman correlation between $\Hzero$ total persistence (VR filtration) and training loss across checkpoints, for each model size. The correlation strengthens monotonically with model size. All correlations are statistically significant ($p < 10^{-5}$).}
    \label{tab:h0_correlation}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Model & Spearman $\rho$ & $p$-value & 95\% Bootstrap CI \\
        \midrule
        \textsc{Tiny-3k}   & $-0.590$ & $5.2 \times 10^{-6}$  & $[-0.796, -0.350]$ \\
        \textsc{Small-7k}  & $-0.854$ & $1.7 \times 10^{-15}$ & $[-0.938, -0.710]$ \\
        \textsc{Med-21k}   & $-0.866$ & $2.3 \times 10^{-16}$ & $[-0.946, -0.713]$ \\
        \textsc{Large-46k} & $-0.900$ & $3.0 \times 10^{-19}$ & $[-0.966, -0.769]$ \\
        \textsc{XL-97k}    & ${\bf -0.907}$ & $4.7 \times 10^{-20}$ & $[-0.970, -0.790]$ \\
        \bottomrule
    \end{tabular}
\end{table}

Our central result is in \Tabref{tab:h0_correlation}: $\Hzero$ total persistence from VR filtration is strongly anticorrelated with training loss across all model sizes.
As training progresses and loss decreases, the weight-space point cloud develops more persistent connected structure.
The correlation ranges from $\rho = -0.59$ in the smallest model (3K parameters) to $\rho = -0.91$ in the largest (97K parameters), and \emph{strengthens monotonically with model size}.
All $p$-values are below $10^{-5}$, and the 95\% bootstrap confidence intervals exclude zero.

This monotonic strengthening is the most promising aspect of the result for scaling applications: the topological signal becomes more informative precisely as model complexity increases---the regime where better monitoring signals are most valuable.

\subsection{Neural Persistence Shows Size-Dependent Behavior}
\label{sec:np_results}

\begin{table}[t]
    \centering
    \caption{Spearman correlation between mean neural persistence and training loss. The sign reverses from negative (small models) to positive (larger models), reflecting a transition from NP tracking weight magnitude concentration to NP tracking weight structure development.}
    \label{tab:np_correlation}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Model & Spearman $\rho$ & $p$-value & Direction \\
        \midrule
        \textsc{Tiny-3k}   & $-0.312$ & $0.026$              & NP $\uparrow$ as loss $\downarrow$ \\
        \textsc{Small-7k}  & $+0.031$ & $0.829$              & Not significant \\
        \textsc{Med-21k}   & $+0.659$ & $1.5 \times 10^{-7}$ & NP $\uparrow$ early, then plateau \\
        \textsc{Large-46k} & $+0.615$ & $1.6 \times 10^{-6}$ & NP tracks loss decline \\
        \textsc{XL-97k}    & $+0.530$ & $6.4 \times 10^{-5}$ & NP tracks loss decline \\
        \bottomrule
    \end{tabular}
\end{table}

Neural persistence shows a more complex relationship with loss (\Tabref{tab:np_correlation}).
In the smallest model (\textsc{Tiny-3k}), higher neural persistence correlates with lower loss ($\rho = -0.31$), consistent with the findings of \citet{rieck2019neural} for simple networks.
However, in larger models the relationship \emph{inverts}: neural persistence and loss are positively correlated ($\rho = +0.53$ to $+0.66$, $p < 10^{-4}$).
This sign reversal reflects a qualitative shift: in single-layer networks, training increases weight magnitude differentiation (increasing NP), while in multi-layer networks, training develops structured weight patterns that increase NP even as neural persistence captures different aspects of the learning dynamics.

\subsection{$\Hone$ Persistence Weakens with Scale}
\label{sec:h1_results}

\begin{table}[t]
    \centering
    \caption{Spearman correlation between $\Hone$ total persistence (loops in VR filtration) and training loss. The correlation weakens with model size, suggesting loop structures are a small-scale phenomenon.}
    \label{tab:h1_correlation}
    \begin{tabular}{@{}lcc@{}}
        \toprule
        Model & Spearman $\rho$ & $p$-value \\
        \midrule
        \textsc{Tiny-3k}   & $-0.641$ & $4.1 \times 10^{-7}$ \\
        \textsc{Small-7k}  & $-0.636$ & $5.2 \times 10^{-7}$ \\
        \textsc{Med-21k}   & $-0.580$ & $8.0 \times 10^{-6}$ \\
        \textsc{Large-46k} & $-0.492$ & $2.5 \times 10^{-4}$ \\
        \textsc{XL-97k}    & $+0.053$ & $0.714$ \\
        \bottomrule
    \end{tabular}
\end{table}

$\Hone$ persistence (loops in the weight-space point cloud) is strongly anticorrelated with loss in smaller models ($\rho = -0.64$) but the relationship weakens and disappears in the largest model ($\rho = +0.05$, not significant; \Tabref{tab:h1_correlation}).
This suggests that loop structures in the weight space are more dynamically relevant at smaller scales and wash out in larger networks, where the higher-dimensional geometry becomes more complex.

\subsection{Topological Features Improve Scaling Predictions}
\label{sec:prediction}

\begin{table}[t]
    \centering
    \caption{Mean absolute error (MAE) of final loss prediction using early training data. TDA features improve over loss-only extrapolation when $\geq$30\% of training data is available. Best results per row in bold.}
    \label{tab:prediction}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Train Fraction & Loss-Only MAE & TDA-Only MAE & Combined MAE & Improvement \\
        \midrule
        20\% & 0.1064 & 0.2634 & 0.1486 & $-39.7\%$ \\
        30\% & 0.1011 & 0.1499 & {\bf 0.0903} & $+10.6\%$ \\
        50\% & 0.0860 & {\bf 0.0554} & 0.0610 & $+29.0\%$ \\
        \bottomrule
    \end{tabular}
\end{table}

We test whether topological features improve prediction of final model loss from early training data (\Tabref{tab:prediction}).
At 20\% of training, TDA features are not yet informative: neural persistence is still in its rapid-change phase, and the TDA-only predictor overestimates loss by 0.26 MAE.
At 30\% of training, the combined predictor achieves the best performance (0.090 MAE), a 10.6\% improvement over loss-only extrapolation.
At 50\% of training, TDA-only prediction actually \emph{outperforms} loss-only extrapolation (0.055 vs.\ 0.086 MAE), and the combined model achieves a 29.0\% improvement.

This pattern is consistent with our correlation results: topological features encode structural information that complements loss curve shape, but they require sufficient training data for the topological signal to develop.

\subsection{Phase Transitions Signal Diminishing Returns}
\label{sec:phase_transitions}

\begin{table}[t]
    \centering
    \caption{Phase transition detection via neural persistence rate of change. Larger models reach diminishing returns later in training, consistent with compute-optimal training principles.}
    \label{tab:phase_transitions}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Model & Max $|\Delta\NP|$ Step & Diminishing Returns Step & Optimal Fraction \\
        \midrule
        \textsc{Tiny-3k}   & 300 (6\%)  & 800 (16\%)  & 16\% \\
        \textsc{Small-7k}  & 300 (6\%)  & 400 (8\%)   & 8\%  \\
        \textsc{Med-21k}   & 300 (6\%)  & 1800 (36\%) & 36\% \\
        \textsc{Large-46k} & 200 (4\%)  & 1500 (30\%) & 30\% \\
        \textsc{XL-97k}    & 300 (6\%)  & 1600 (32\%) & 32\% \\
        \bottomrule
    \end{tabular}
\end{table}

We detect phase transitions in training dynamics by tracking the rate of change of neural persistence (\Tabref{tab:phase_transitions}).
All models show a rapid-change phase in the first 4--6\% of training (corresponding to the learning rate warmup period), followed by gradually diminishing changes.
The ``diminishing returns'' point---where $|\Delta\NP|$ drops below 10\% of its peak---occurs at 8--16\% of training for the two smallest models but at 30--36\% for the three larger models.
This is consistent with compute-optimal training principles~\citep{hoffmann2022training}: larger models benefit from longer training, and topological features can detect the point at which marginal improvements slow.

\subsection{Topological Features Scale with Model Size}
\label{sec:tda_scaling}

\begin{table}[t]
    \centering
    \caption{Power-law scaling of features with model size $N$. $\Hzero$ total persistence scales significantly with model size, while neural persistence does not.}
    \label{tab:scaling}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        Feature & Exponent & $R^2$ & Spearman $\rho$ & $p$-value \\
        \midrule
        Final loss              & $-0.144$ & 0.997 & $-1.00$ & $<10^{-24}$ \\
        $\Hzero$ total persist. & $+0.204$ & 0.852 & $+0.90$ & $0.037$ \\
        Neural persistence      & $-0.024$ & 0.266 & $-0.30$ & $0.624$ \\
        \bottomrule
    \end{tabular}
\end{table}

$\Hzero$ total persistence at the final checkpoint scales as a power law with model size: $\TP_0(N) \propto N^{0.204}$ with $R^2 = 0.852$ and $\rho = 0.90$ ($p = 0.037$; \Tabref{tab:scaling}).
Larger models develop more persistent topological structure in their weight spaces.
Neural persistence does not show a statistically significant scaling relationship ($p = 0.624$), though the sample size ($n = 5$) limits statistical power.

\subsection{Validation on \pythia-14m}
\label{sec:pythia}

\begin{table}[t]
    \centering
    \caption{Topological features across \pythia-14m training. Neural persistence increases monotonically ($+88\%$) while $\Hzero$ total persistence decreases ($-35\%$), confirming trends from our small models.}
    \label{tab:pythia}
    \begin{tabular}{@{}rcccc@{}}
        \toprule
        Step & NP Mean & NP Std & $\Hzero$ Total \\
        \midrule
        0       & 0.338 & 0.029 & 69.7 \\
        1,000   & 0.377 & 0.067 & 69.4 \\
        5,000   & 0.484 & 0.132 & 70.9 \\
        10,000  & 0.531 & 0.147 & 69.8 \\
        50,000  & 0.611 & 0.146 & 54.4 \\
        100,000 & 0.627 & 0.140 & 46.1 \\
        143,000 & 0.636 & 0.132 & {\bf 45.4} \\
        \bottomrule
    \end{tabular}
\end{table}

We validate our findings on \pythia-14m, a 14M-parameter GPT-NeoX model trained on The Pile~\citep{biderman2023pythia}.
\Tabref{tab:pythia} shows that the same qualitative trends hold:
(1) neural persistence increases monotonically from 0.338 to 0.636 ($+88\%$), confirming the positive NP--training progress relationship observed in our larger custom models;
(2) $\Hzero$ total persistence \emph{decreases} from 69.7 to 45.4 ($-35\%$), indicating the weight space becomes more connected during training;
(3) the rate of NP change diminishes over training, consistent with the diminishing returns signal detected in our models.
These results confirm that the topological signatures we identify in small controlled experiments generalize to real transformer training at the 14M-parameter scale.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/summary_figure.png}
    \caption{Overview of main results. \figtop \figleft: Training loss curves for all five model sizes, showing expected power-law scaling. \figtop \figright: Neural persistence evolution during training, with larger models showing higher final NP. \figbottom \figleft: $\Hzero$ total persistence vs.\ training loss, showing the strong anticorrelation that strengthens with model size. \figbottom \figright: Scaling of topological features with model size on a log-log scale.}
    \label{fig:summary}
\end{figure}
